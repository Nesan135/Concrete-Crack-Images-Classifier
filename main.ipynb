{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Formulation"
      ],
      "metadata": {
        "id": "WEv5ecoCy2S9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The dataset contains concrete images having cracks. The task at hand is to classify if a image is negative or positive in terms of having cracks.\n",
        "\n",
        "2. The dataset is divided into two classes, Negative and Positive crack images.\n",
        "It must be divided evenly into train, validation and test batches. The Negative and Positive images also need to be shuffled together into a single batch.\n",
        "\n",
        "3. Each class has 20000 images with 227 x 227 pixels with RGB channels.\n",
        "These images must be resized to fit the maximum of 224 x 224 pixels required by MobileNetV2.\n",
        "\n",
        "4. No data augmentation in terms of random rotation or flipping is applied.\n",
        "It will be applied later with an augmentation layer.\n",
        "\n",
        "5. Apply Transfer learning with MobileNetV2 then do fine-tuning."
      ],
      "metadata": {
        "id": "aBpGCLfKy4Mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "Q2wrT2AAdys8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "# !pip install pyunpack\n",
        "# !pip install patool\n",
        "# !pip install split-folders\n",
        "\n",
        "import splitfolders,os,random,cv2,datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pyunpack import Archive\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,optimizers,losses,metrics,callbacks,applications\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# extract zip file downloaded from reference [1]\n",
        "!mkdir raw\n",
        "RAW_PATH = os.path.join(os.getcwd(),'raw')\n",
        "RAR_PATH = os.path.join(os.getcwd(),'Concrete Crack Images for Classification.rar')\n",
        "Archive(RAR_PATH).extractall(RAW_PATH)\n",
        "\n",
        "# split negative and positive folders to train, test, split\n",
        "splitfolders.ratio(RAW_PATH, output=\"dataset\", seed=1337, ratio=(.7, 0.2,0.1))\n",
        "DATA_PATH = os.path.join(os.getcwd(),'dataset')\n",
        "\n",
        "# define paths\n",
        "train_path = DATA_PATH+'/train'\n",
        "val_path = DATA_PATH+'/val'\n",
        "test_path = DATA_PATH+'/test'\n",
        "\n",
        "# constants\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (224,224)\n",
        "SEED = 123\n",
        "\n",
        "# resize and shuffle\n",
        "train_dataset = keras.utils.image_dataset_from_directory(train_path,shuffle=True,batch_size=BATCH_SIZE,image_size=IMG_SIZE,seed=None)\n",
        "validation_dataset = keras.utils.image_dataset_from_directory(val_path,shuffle=True,batch_size=BATCH_SIZE,image_size=IMG_SIZE,seed=None)\n",
        "test_dataset = keras.utils.image_dataset_from_directory(test_path,shuffle=True,batch_size=BATCH_SIZE,image_size=IMG_SIZE,seed=None)\n",
        "\n",
        "# review some images from train dataset\n",
        "class_names = train_dataset.class_names\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "for images,labels in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        plt.subplot(3,3,i+1)\n",
        "        plt.imshow(images[i].numpy().astype('uint8'))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis('off')\n",
        "\n",
        "# Convert the BatchDataset into PrefetchDataset\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "pf_train = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "pf_val = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "pf_test = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print('Number of train batches: %d' % tf.data.experimental.cardinality(train_dataset))\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))\n",
        "\n",
        "# Create a small pipeline for data augmentation\n",
        "data_augmentation = keras.Sequential()\n",
        "data_augmentation.add(layers.RandomFlip('horizontal',seed=SEED))\n",
        "data_augmentation.add(layers.RandomRotation(0.2,seed=SEED))"
      ],
      "metadata": {
        "id": "VYqe0koGdKCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Development"
      ],
      "metadata": {
        "id": "TQMEndFhymu4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu57bRLlecr3"
      },
      "outputs": [],
      "source": [
        "# Prepare the layer for data preprocessing\n",
        "preprocess_input = applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "# Apply transfer learning\n",
        "IMG_SHAPE = (224,224,3)\n",
        "feature_extractor = applications.MobileNetV2(input_shape=IMG_SHAPE,include_top=False,weights='imagenet')\n",
        "\n",
        "# Disable the training for the feature extractor (freeze the layers)\n",
        "feature_extractor.trainable = False\n",
        "feature_extractor.summary()\n",
        "keras.utils.plot_model(feature_extractor,show_shapes=True)\n",
        "\n",
        "# Create the classification layers\n",
        "global_avg = layers.GlobalAveragePooling2D()\n",
        "l2 = keras.regularizers.L2()\n",
        "output_layer = layers.Dense(len(class_names),activation='softmax',kernel_regularizer=l2)\n",
        "\n",
        "# Use functional API to link all of the modules together\n",
        "inputs = keras.Input(shape=IMG_SHAPE)\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess_input(x)\n",
        "x = feature_extractor(x, training=False)\n",
        "x = global_avg(x)\n",
        "x = layers.Dense(300, activation=tf.keras.activations.relu)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = output_layer(x)\n",
        "model = keras.Model(inputs=inputs,outputs=outputs)\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = optimizers.Adam(learning_rate=0.0001)\n",
        "loss = losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])\n",
        "\n",
        "# initial evaluation\n",
        "loss0, accuracy0 = model.evaluate(validation_dataset)\n",
        "print(\"initial loss: {:.2f}\".format(loss0))\n",
        "print(\"initial accuracy: {:.2f}\".format(accuracy0))\n",
        "\n",
        "# specify callbacks\n",
        "log_path = os.path.join('log_dir',datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
        "tb = callbacks.TensorBoard(log_dir=log_path)\n",
        "\n",
        "# Train the model\n",
        "EPOCHS = 1\n",
        "history = model.fit(pf_train,validation_data=pf_val,epochs=EPOCHS,callbacks=[tb])\n",
        "\n",
        "# unfreeze all layers and freeze only early layers\n",
        "feature_extractor.trainable = True\n",
        "for layer in feature_extractor.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# compile the model\n",
        "optimizer = optimizers.RMSprop(learning_rate=0.00001)\n",
        "model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])\n",
        "\n",
        "fine_tune_epoch = 5\n",
        "history_fine = model.fit(pf_train,validation_data=pf_val,epochs=fine_tune_epoch,initial_epoch=history.epoch[-1],callbacks=[tb])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot performance of model and save image\n",
        "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
        "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss'] + history_fine.history['loss']\n",
        "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Train')\n",
        "plt.plot(val_acc, label='Validation')\n",
        "plt.ylim([0.996, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Train')\n",
        "plt.plot(val_loss, label='Validation')\n",
        "plt.ylim([0, 0.075])\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.savefig('Performance.png')\n",
        "plt.show()\n",
        "\n",
        "# plot model architecture and save image\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# save model\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "8XtIMMvc6LDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Deployment"
      ],
      "metadata": {
        "id": "2gZIDDuoytfP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihs300pCi0qw"
      },
      "outputs": [],
      "source": [
        "# Evaluate the final model\n",
        "test_loss,test_acc = model.evaluate(pf_test)\n",
        "\n",
        "print(\"Loss = \",test_loss)\n",
        "print(\"Accuracy = \",test_acc)\n",
        "\n",
        "# Deploy the model using the test data\n",
        "image_batch, label_batch = pf_test.as_numpy_iterator().next()\n",
        "predictions = np.argmax(model.predict(image_batch),axis=1)\n",
        "\n",
        "# Compare label and prediction\n",
        "print('Predictions:\\n', predictions)\n",
        "print('Labels:\\n', label_batch)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "  ax = plt.subplot(3, 3, i + 1)\n",
        "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
        "  plt.title(class_names[predictions[i]])\n",
        "  plt.axis(\"off\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WEv5ecoCy2S9",
        "Q2wrT2AAdys8",
        "TQMEndFhymu4",
        "2gZIDDuoytfP"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}